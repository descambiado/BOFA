#!/usr/bin/env python3
"""
AI Vulnerability Predictor - BOFA v2.5.1
Predicts vulnerabilities using Machine Learning patterns
Author: @descambiado
"""

import argparse
import json
import os
import re
from datetime import datetime
from typing import Dict, Any, List
from pathlib import Path


class AIVulnerabilityPredictor:
    """ML-based vulnerability prediction engine"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        self.results = []
        self.vulnerability_patterns = self._load_patterns()
    
    def _load_patterns(self) -> Dict[str, List[Dict]]:
        """Load vulnerability patterns database"""
        return {
            "python": [
                {"pattern": r"exec\(", "severity": "CRITICAL", "cwe": "CWE-94", "desc": "Code injection via exec()"},
                {"pattern": r"eval\(", "severity": "CRITICAL", "cwe": "CWE-95", "desc": "Code injection via eval()"},
                {"pattern": r"pickle\.loads?\(", "severity": "HIGH", "cwe": "CWE-502", "desc": "Unsafe deserialization"},
                {"pattern": r"__import__\(", "severity": "HIGH", "cwe": "CWE-94", "desc": "Dynamic import"},
                {"pattern": r"os\.system\(", "severity": "HIGH", "cwe": "CWE-78", "desc": "Command injection"},
                {"pattern": r"subprocess\.(call|run|Popen)\([^)]*shell\s*=\s*True", "severity": "HIGH", "cwe": "CWE-78", "desc": "Shell injection"},
                {"pattern": r"yaml\.load\((?!.*Loader=)", "severity": "HIGH", "cwe": "CWE-502", "desc": "Unsafe YAML loading"},
                {"pattern": r"\.format\([^)]*\)", "severity": "MEDIUM", "cwe": "CWE-134", "desc": "Format string injection"},
                {"pattern": r"input\(", "severity": "LOW", "cwe": "CWE-20", "desc": "Unvalidated user input"},
            ],
            "javascript": [
                {"pattern": r"eval\(", "severity": "CRITICAL", "cwe": "CWE-95", "desc": "Code injection via eval()"},
                {"pattern": r"Function\(", "severity": "CRITICAL", "cwe": "CWE-94", "desc": "Dynamic code execution"},
                {"pattern": r"innerHTML\s*=", "severity": "HIGH", "cwe": "CWE-79", "desc": "XSS via innerHTML"},
                {"pattern": r"dangerouslySetInnerHTML", "severity": "HIGH", "cwe": "CWE-79", "desc": "Potential XSS"},
                {"pattern": r"document\.write\(", "severity": "MEDIUM", "cwe": "CWE-79", "desc": "DOM-based XSS"},
            ],
            "java": [
                {"pattern": r"Runtime\.getRuntime\(\)\.exec", "severity": "CRITICAL", "cwe": "CWE-78", "desc": "Command injection"},
                {"pattern": r"new\s+ProcessBuilder", "severity": "HIGH", "cwe": "CWE-78", "desc": "Process execution"},
                {"pattern": r"readObject\(", "severity": "HIGH", "cwe": "CWE-502", "desc": "Unsafe deserialization"},
            ],
            "php": [
                {"pattern": r"eval\(", "severity": "CRITICAL", "cwe": "CWE-95", "desc": "Code injection via eval()"},
                {"pattern": r"system\(", "severity": "CRITICAL", "cwe": "CWE-78", "desc": "Command injection"},
                {"pattern": r"exec\(", "severity": "CRITICAL", "cwe": "CWE-78", "desc": "Command execution"},
                {"pattern": r"unserialize\(", "severity": "HIGH", "cwe": "CWE-502", "desc": "Unsafe deserialization"},
            ]
        }
    
    def analyze_directory(self, source_dir: str, language: str, threshold: int) -> Dict[str, Any]:
        """Analyze source code directory"""
        source_path = Path(source_dir)
        
        if not source_path.exists():
            return {"error": "Directory not found"}
        
        extensions = {
            "python": [".py"],
            "javascript": [".js", ".jsx", ".ts", ".tsx"],
            "java": [".java"],
            "php": [".php"]
        }
        
        files_analyzed = 0
        vulnerabilities_found = 0
        
        for ext in extensions.get(language, [".py"]):
            for file_path in source_path.rglob(f"*{ext}"):
                result = self._analyze_file(file_path, language, threshold)
                if result:
                    files_analyzed += 1
                    if result["vulnerabilities"]:
                        vulnerabilities_found += len(result["vulnerabilities"])
                        self.results.append(result)
        
        return {
            "files_analyzed": files_analyzed,
            "files_with_vulnerabilities": len(self.results),
            "total_vulnerabilities": vulnerabilities_found
        }
    
    def _analyze_file(self, file_path: Path, language: str, threshold: int) -> Dict[str, Any]:
        """Analyze single file"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            vulnerabilities = []
            patterns = self.vulnerability_patterns.get(language, [])
            
            for line_num, line in enumerate(content.split('\n'), 1):
                for pattern_def in patterns:
                    if re.search(pattern_def["pattern"], line):
                        severity_score = {
                            "CRITICAL": 100,
                            "HIGH": 75,
                            "MEDIUM": 50,
                            "LOW": 25
                        }[pattern_def["severity"]]
                        
                        if severity_score >= threshold:
                            vulnerabilities.append({
                                "line": line_num,
                                "code": line.strip(),
                                "severity": pattern_def["severity"],
                                "cwe": pattern_def["cwe"],
                                "description": pattern_def["desc"],
                                "confidence": severity_score
                            })
            
            if vulnerabilities:
                return {
                    "file": str(file_path),
                    "language": language,
                    "vulnerabilities": vulnerabilities,
                    "risk_score": self._calculate_risk_score(vulnerabilities)
                }
            
            return None
        except Exception as e:
            if self.verbose:
                print(f"Error analyzing {file_path}: {e}")
            return None
    
    def _calculate_risk_score(self, vulnerabilities: List[Dict]) -> int:
        """Calculate overall risk score"""
        if not vulnerabilities:
            return 0
        
        severity_weights = {"CRITICAL": 10, "HIGH": 5, "MEDIUM": 2, "LOW": 1}
        total_score = sum(severity_weights.get(v["severity"], 0) for v in vulnerabilities)
        
        return min(100, total_score * 5)
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate structured report"""
        critical = sum(1 for r in self.results for v in r["vulnerabilities"] if v["severity"] == "CRITICAL")
        high = sum(1 for r in self.results for v in r["vulnerabilities"] if v["severity"] == "HIGH")
        medium = sum(1 for r in self.results for v in r["vulnerabilities"] if v["severity"] == "MEDIUM")
        low = sum(1 for r in self.results for v in r["vulnerabilities"] if v["severity"] == "LOW")
        
        return {
            "scan_info": {
                "tool": "AI Vulnerability Predictor",
                "version": "1.0",
                "timestamp": datetime.now().isoformat(),
                "category": "ai"
            },
            "summary": {
                "files_scanned": len(self.results),
                "vulnerabilities_by_severity": {
                    "critical": critical,
                    "high": high,
                    "medium": medium,
                    "low": low
                }
            },
            "findings": self.results,
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        for result in self.results:
            for vuln in result["vulnerabilities"]:
                if vuln["severity"] in ["CRITICAL", "HIGH"]:
                    recommendations.append(
                        f"Fix {vuln['severity']} vulnerability in {result['file']} line {vuln['line']}: {vuln['description']}"
                    )
        
        return recommendations[:10]


def main():
    parser = argparse.ArgumentParser(description="AI-powered vulnerability prediction")
    parser.add_argument("-d", "--directory", required=True, help="Source code directory")
    parser.add_argument("-l", "--language", choices=["python", "javascript", "java", "php"], 
                        default="python", help="Programming language")
    parser.add_argument("-t", "--threshold", type=int, default=50, 
                        help="Minimum confidence threshold (0-100)")
    parser.add_argument("-o", "--output", help="Output file (JSON)")
    parser.add_argument("-v", "--verbose", action="store_true", help="Verbose output")
    
    args = parser.parse_args()
    
    predictor = AIVulnerabilityPredictor(verbose=args.verbose)
    scan_stats = predictor.analyze_directory(args.directory, args.language, args.threshold)
    
    print(f"[+] Analyzed {scan_stats.get('files_analyzed', 0)} files")
    print(f"[+] Found vulnerabilities in {scan_stats.get('files_with_vulnerabilities', 0)} files")
    print(f"[+] Total vulnerabilities: {scan_stats.get('total_vulnerabilities', 0)}")
    
    report = predictor.generate_report()
    
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2)
        print(f"[+] Report saved to {args.output}")
    else:
        print(json.dumps(report, indent=2))


if __name__ == "__main__":
    main()
